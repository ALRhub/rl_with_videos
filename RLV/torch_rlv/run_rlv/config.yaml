---
name: "SLURM"   # MUST BE "SLURM", is used for communicating with the cluster scheduler

# Required
partition: "single"
job-name: "i3rl"    # this will be the experiments name in slurm

# Required - Cluster Specific. These defaults almost always work
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 8
mem-per-cpu: 8000
time: 1440  # in minutes

sbatch_args:    # Dictionary of SBATCH keywords and arguments
  distribution: cyclic  # To have repetitions of the same exp be distributed to different CPU nodes

---
# DEFAULT parameters (Optional)
name: "DEFAULT"   # MUST BE DEFAULT

# Implementation default parameters
path: "./output/"   # location to save results in
repetitions: 1   # number of times one set of parameters is run

params:
  # this is where you specify all parameters needed for your experiment.
  # the tree structure you define here will be translated to a nested python dictionary
  action_space_type: 'discrete'
  env_name: 'acrobot'
  algo_name: 'sac'
  n_actions: 250
  pre_steps: 1000
  layer1_size: 256
  layer2_size: 256
  lr: 0.003

---
name: "my_run"
path: "./output/"
repetitions: 1   # repeat 8 times

params:
  action_space_type: 'discrete'
  env_name: 'acrobot'
  algo_name: 'rlv'
  n_games: 80000
  pre_steps: 0
  layer1_size: 256
  layer2_size: 256
  lr: 0.003

---
name: "rlv"
path: "./output/"
repetitions: 1   # repeat 8 times

params:
  action_space_type: 'discrete'
  env_name: 'acrobot'
  algo_name: 'rlv'
  n_games: 1
  pre_steps: 0
  layer1_size: 256
  layer2_size: 256
  lr: 0.003